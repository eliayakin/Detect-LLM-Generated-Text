{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":61542,"databundleVersionId":7516023},{"sourceType":"datasetVersion","sourceId":7489386,"datasetId":4332461,"databundleVersionId":7582117},{"sourceType":"datasetVersion","sourceId":6977472,"datasetId":4005256,"databundleVersionId":7064285},{"sourceType":"datasetVersion","sourceId":4620664,"datasetId":2663421,"databundleVersionId":4682266},{"sourceType":"datasetVersion","sourceId":6920046,"datasetId":3973543,"databundleVersionId":7006264},{"sourceType":"datasetVersion","sourceId":7409757,"datasetId":4309703,"databundleVersionId":7500977}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !rm -rf /kaggle/working/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python --version  \n# Python 3.10.12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from typing import Optional\n\nimport os\nimport pickle\nimport re\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport scipy\nfrom scipy.sparse import spmatrix\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom collections import Counter\n\n!pip install /kaggle/input/pyspellchecker/pyspellchecker-0.8.0-py3-none-any.whl\nfrom spellchecker import SpellChecker\n\nimport spacy\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Globals","metadata":{}},{"cell_type":"code","source":"CSV_INPUT_PATH = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv'\nINPUT_PATH = '/kaggle/input/'\nCATBOOST_PATH = '/kaggle/input/catboost/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_run_time(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f'The function {func.__name__} took {time.time() - start_time:.3f} seconds')\n        return result\n    return wrapper","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def correct_text_spelling(text: str) -> str:\n    spell = SpellChecker()\n    words = re.findall(r'\\b\\w+\\b', text)\n    misspelled = spell.unknown(words)\n    corrected_text = text\n    for word in misspelled:\n        if spell.correction(word):\n            corrected_text = corrected_text.replace(word, spell.correction(word))\n    return corrected_text\n\ndef correct_spelling(df: pd.DataFrame, text_col: str = 'text') -> pd.DataFrame:\n    df_ = df.copy()\n    df_['corrected'] = df_[text_col].apply(correct_text_spelling)\n    return df_['corrected']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features","metadata":{}},{"cell_type":"markdown","source":"## Text Cleaning and Features","metadata":{}},{"cell_type":"code","source":"def get_corpus(df: pd.DataFrame, text_col: str = 'clean_text') -> list[str]:\n    return df[text_col].to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(df: pd.DataFrame) -> pd.DataFrame:\n    df_ = df.copy()\n    \n    df_['clean_text'] = (df_['text'].str.replace('\\n\\n', '') \n                                    .str.replace('\\'s', '')\n                                    .str.replace('[.,?!:;\\'\\\\\\\\\"]', '', regex=True)\n                                    .str.lower()\n                        )\n    \n    return df_['clean_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tfidf(corpus: list[str], vectorizer: Optional[TfidfVectorizer] = None) -> [scipy.sparse.spmatrix | tuple[scipy.sparse.spmatrix, TfidfVectorizer]]:\n    if vectorizer:\n        M = vectorizer.transform(corpus)\n        return M\n    else:\n        vectorizer = TfidfVectorizer(\n                                     lowercase=False,\n                                     sublinear_tf=True,\n                                     stop_words='english',\n                                     ngram_range=(3,5)\n                                    )\n        \n        M = vectorizer.fit_transform(corpus)\n        \n        return M, vectorizer ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_feature(df: pd.DataFrame, text_col: str = 'clean_text') -> pd.DataFrame:\n    df_ = df.copy()\n    \n    nlp = spacy.load(\"en_core_web_sm\")\n\n    docs = [nlp(t) for t in df_[text_col]]\n    pos_tags = [[token.pos_ for token in doc] for doc in docs]\n    \n    df_['pos_tags'] = pos_tags\n    \n    df_['pos_text'] = df_['pos_tags'].apply(lambda x: ' '.join(x))\n    \n    df_['NOUN_count'] = df_['pos_text'].str.count('NOUN')\n    df_['VERB_count'] = df_['pos_text'].str.count('VERB')\n    df_['ADJ_count'] = df_['pos_text'].str.count('ADJ')\n    df_['ADV_count'] = df_['pos_text'].str.count('ADV')\n    df_['ADP_count'] = df_['pos_text'].str.count('ADP')\n    df_['PRON_count'] = df_['pos_text'].str.count('PRON')\n    df_['PROPN_count'] = df_['pos_text'].str.count('PROPN')\n    df_['PUNCT_count'] = df_['pos_text'].str.count('PUNCT')\n    df_['AUX_count'] = df_['pos_text'].str.count('AUX')\n    df_['NUM_count'] = df_['pos_text'].str.count('NUM')\n    df_['X_count'] = df_['pos_text'].str.count('X')\n    \n    df_ = df_.drop(columns=['pos_tags', 'pos_text'])\n    \n    return df_\n    \ndef stopwords_feature(df: pd.DataFrame, text_col: str = 'clean_text') -> pd.DataFrame:\n    df_ = df.copy()\n    \n    stop_words = set(stopwords.words('english'))\n\n    def stopwords_counter_and_filter(row):\n        words = word_tokenize(row[text_col])\n        stopword_counter = sum(1 for w in words if w.lower() in stop_words)\n        filtered_text = ' '.join(w for w in words if w.lower() not in stop_words)\n        return stopword_counter, filtered_text\n\n    df_[['number_of_stopwords', 'filtered_text']] = df_.apply(stopwords_counter_and_filter, axis=1, result_type='expand')\n\n    return df_\n\ndef caps_to_periods_ratio_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df_ = df.copy()\n    \n    parentheses_count = df_['parentheses_count']\n    caps_count = df_['text'].str.count(r'[A-Z]')\n    df_['caps_to_periods_ratio'] = caps_count / parentheses_count.replace(0, 1) # replace 0 by 1 to avoid deviding by 0\n    \n    return df_['caps_to_periods_ratio']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Extraction","metadata":{}},{"cell_type":"code","source":"def feature_extraction(df: pd.DataFrame, text_col: str = 'clean_text') -> pd.DataFrame:\n    \"\"\" Extracts numeric features based on the chosen text column. \"\"\"\n    df_ = df.copy()\n    \n    # text length\n    df_['num_of_words'] = (df_[text_col].str.replace('\\n\\n', '') # removing row\n                                .str.count(' ')+1)\n    # uniqe words per row\n    df_['text_vocab_size'] = df_[text_col].apply(lambda x : len(set(x.split())))\n    \n    df_['num_of_sentences'] = df_[text_col].str.count('\\.')\n    \n    df_['parentheses_count'] = df_['text'].str.count('\\(|\\)')\n    \n    df_['semicolon_count'] = df_['text'].str.count(';')\n\n    df_['hypen_count'] = df_['text'].str.count('-')\n    \n    df_['dash_count'] = df_['text'].str.count('—')\n    \n    df_['comma_count'] = df_['text'].str.count(',')\n    \n    df_['qm_count'] = df_['text'].str.count('\\?')\n    \n    df_['en_count'] = df_['text'].str.count('!')\n    \n    df_['apostrophe_count'] = df_['text'].str.count('’')\n    \n    df_['paragraph_count'] = df_['text'].str.count('\\n\\n')\n    \n    # extract POS features\n    df_ = pos_feature(df_, text_col)\n    \n    # df_ = stopwords_feature(df_, text_col)\n    \n    # count typos\n    # df_ = typos_count_feature(df_)\n\n    # calculating the ratio between the number of capital letters and periods\n    df_['caps_to_periods_ratio'] = caps_to_periods_ratio_feature(df_)\n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"def get_df_test() -> pd.DataFrame:\n    \"\"\" Returns a dataframe with cleaned text. \"\"\"\n    X_test = pd.read_csv(CSV_INPUT_PATH)\n    # X_test['text'] = correct_spelling(X_test, 'text')\n    X_test['clean_text'] = clean_text(X_test)\n    return X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading pickles and CSVs","metadata":{}},{"cell_type":"code","source":"files = os.listdir(CATBOOST_PATH)\n\nfor file_name in files:\n    file_path = os.path.join(CATBOOST_PATH, file_name)\n    name, extension = os.path.splitext(file_name)\n    with open(file_path, 'rb') as file:\n        if extension == '.pickle':\n            globals()[name] = pickle.load(file)\n        elif extension == '.csv':\n            globals()[name] = pd.read_csv(file, index_col=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models by Feature Sets","metadata":{}},{"cell_type":"code","source":"@print_run_time\ndef text_tfidf_models_test() -> np.array:\n    X_test = get_df_test()\n\n    cols = ['id', 'prompt_id']\n    X_test = X_test.drop(columns=cols)\n    \n    corpus_test = get_corpus(X_test)\n    tfidf_X_test = tfidf(corpus_test, text_tfidf_vectorizer)\n    tfidf_test_pred = text_tfidf_ensemble.predict_proba(tfidf_X_test)[:,1]\n    \n    return tfidf_test_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@print_run_time\ndef pos_tfidf_models_test() -> np.array:\n    X_test = get_df_test()\n\n    # extracting pos-related features\n    nlp = spacy.load(\"en_core_web_sm\")\n    docs = [nlp(t) for t in X_test['clean_text']]\n    pos_tags = [[token.pos_ for token in doc] for doc in docs]\n    X_test['pos_tags'] = pos_tags\n    X_test['pos_text'] = X_test['pos_tags'].apply(lambda x: ' '.join(x))\n\n    cols = ['id', 'prompt_id']\n    X_test = X_test.drop(columns=cols)\n    \n    pos_corpus_test = get_corpus(X_test, text_col='pos_text')\n    pos_tfidf_X_test = tfidf(pos_corpus_test, pos_tfidf_vectorizer)\n    pos_test_pred = pos_tfidf_ensemble.predict_proba(pos_tfidf_X_test)[:,1]\n    \n    return pos_test_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@print_run_time\ndef numeric_features_models_test() -> tuple[pd.DataFrame, np.array]:   \n    X_test = get_df_test()\n\n    # extracting numeric features\n    X_test = feature_extraction(X_test, 'clean_text')\n\n    X_test_id = X_test['id']\n\n    cols = ['id', 'prompt_id', 'text','clean_text']\n    X_test = X_test.drop(columns=cols)\n    \n    test_pred = num_ensemble.predict_proba(X_test)[:,1]\n    \n    return X_test_id, test_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Execution and Analysis","metadata":{}},{"cell_type":"code","source":"tfidf_test_pred = text_tfidf_models_test()\npos_test_pred = pos_tfidf_models_test()\nX_test_id, test_pred = numeric_features_models_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test = pd.DataFrame({'tfidf_pred': tfidf_test_pred, 'pos_pred': pos_test_pred, 'num_pred': test_pred})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred = cat_clf_stack.predict_proba(pred_test)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({'id': X_test_id, 'generated': y_test_pred})\nresults.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
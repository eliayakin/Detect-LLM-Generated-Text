{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":6920046,"sourceType":"datasetVersion","datasetId":3973543},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7370702,"sourceType":"datasetVersion","datasetId":4282426}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python --version  \n# Python 3.10.12","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\n\nimport torch\n\nimport numpy as np\nimport pandas as pd\n\n# !pip install sentencepiece\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n!pip install datasets\n\nimport datasets\nfrom datasets import Dataset\n\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Globals","metadata":{}},{"cell_type":"code","source":"SEED = 1337 # 45090448\nVALIDATION_SHARE = 0.3\nOUTPUT_PATH = '/kaggle/working/'\nTRAIN_CSV_INPUT_PATH = \"'/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\"\nTEST_CSV_INPUT_PATH = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-large\"\nmax_length = 512\nmodel_name = \"deberta-v3-large\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df() -> pd.DataFrame:\n    daigt = pd.read_csv(TRAIN_CSV_INPUT_PATH)\n    daigt.rename(columns={'label': 'generated'}, inplace=True)\n    df_ = daigt[daigt['RDizzl3_seven'] == True] # only True ones\n    cols = ['text', 'generated']\n    df_ = df_[cols]\n    return df_\n\ndef ds_prep(df: pd.DataFrame, y: pd.Series) -> Dataset:\n    df_ = df.copy()\n    df_ = (df_.loc[y.index.to_list()]\n              .rename(columns={'generated': 'label'})\n          )\n    \n    return Dataset.from_pandas(df_)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n    auc = roc_auc_score(labels, probs[:,1], multi_class='ovr')\n    return {\"roc_auc\": auc}\n\ndef get_probas_with_trainer(ds_enc: Dataset) -> Dataset:\n    \n    ds_preds = trainer.predict(ds_enc)\n    logits = ds_preds.predictions\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n    \n    # keeping only positives proba  \n    return probs[:,1] \n\ndef to_df_from_ds(ds: Dataset, y: pd.DataFrame) -> pd.DataFrame:\n    \n    df_ = ds.to_pandas()\n    df_.index(y.index, inplace=True)\n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = get_df()\n\ndf = df.rename(columns={'generated': 'label'})\ny = df['label']\n\ndf_tr, df_val = train_test_split(df, test_size=VALIDATION_SHARE ,stratify=y, random_state=SEED)\n\n# creating Dataset format \nds_tr = Dataset.from_pandas(df_tr)\nds_val = Dataset.from_pandas(df_val)\n\ntrain_obs = df_tr.shape[0]\nval_obs = df_val.shape[0]\n\ndel df_tr, df_val\ngc.collect()\n\n# loading tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n\ndef preprocess_function(ds: Dataset, text_col: str = 'text'):\n    return tokenizer(ds[text_col], max_length=max_length, padding=True, truncation=True)\n\n# encoding\nds_tr_enc = ds_tr.map(preprocess_function, batched=True)\n\nds_val_enc = ds_val.map(preprocess_function, batched=True)\n\n# choosing pretrained model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n\n# training loop params\nmetric_name = \"roc_auc\"\ntrain_batch_size = 2 # 8\neval_batch_size = 8 # 32\ngrad_acc = 4\nnum_steps = train_obs // (train_batch_size * grad_acc)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nargs = TrainingArguments(\n    f\"{model_name}-finetuned\",\n    evaluation_strategy = \"steps\",\n    save_strategy = \"steps\",\n    eval_steps = num_steps // 3,\n    save_steps = num_steps // 3,\n    learning_rate=2e-5,\n    per_device_train_batch_size=train_batch_size,\n    per_device_eval_batch_size=eval_batch_size,\n    gradient_accumulation_steps=grad_acc,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    load_best_model_at_end=False,\n    metric_for_best_model=metric_name,\n    report_to='none' # change to wandb when internet access is enabled\n)\n\ntrainer = Trainer(\n    model.to(device),\n    args,\n    train_dataset=ds_tr_enc,\n    eval_dataset=ds_val_enc,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\ntrainer.save_model(OUTPUT_PATH)\n\ndef get_probas_with_trainer(ds_enc: Dataset) -> Dataset:\n    \n    ds_preds = trainer.predict(ds_enc)\n    logits = ds_preds.predictions\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n    \n    # keeping only positives proba  \n    return probs[:,1] \n\nds_tr_proba = get_probas_with_trainer(ds_tr_enc)\nds_val_proba = get_probas_with_trainer(ds_val_enc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(TEST_CSV_INPUT_PATH)\n\ndf_test = df_test[['id', 'text']]\ndf_test_id = df_test['id']\n\nds_test = Dataset.from_pandas(df_test)\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n\nds_test_enc = ds_test.map(preprocess_function, batched=True)\nds_test_proba = get_probas_with_trainer(ds_test_enc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({'id': df_test_id, 'generated': ds_test_proba})\nresults.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}
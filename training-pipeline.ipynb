{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":6920046,"sourceType":"datasetVersion","datasetId":3973543},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7409757,"sourceType":"datasetVersion","datasetId":4309703}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !rm -rf /kaggle/working/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python --version  \n# Python 3.10.12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our solution is a two-layer stacked ensemble model. The first layer consists of three ensembles applied on three different sets of features, which are then fed to a gradient boosting algorithm.  ","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from typing import Optional, Union\nimport time\n\nimport os\nimport pickle\nimport gc\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom collections import Counter\n\n!pip show pyspellchecker || pip install /kaggle/input/pyspellchecker/pyspellchecker-0.8.0-py3-none-any.whl\nfrom spellchecker import SpellChecker\n\nimport spacy\n\nimport scipy\nfrom scipy.sparse import spmatrix\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n# from sklearn.preprocessing import MinMaxScaler\n\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Globals","metadata":{}},{"cell_type":"code","source":"SEED = 1337 # 45090448\nVALIDATION_SHARE = 0.4\nSAMPLE = True # if True then df.sample \nSAMPLE_RATE = 0.002\nCSV_INPUT_PATH = '/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv'\nINPUT_PATH = '/kaggle/input/'\nOUTPUT_PATH = '/kaggle/working/'\nMAX_ITER = 1200","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_run_time(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f'The function {func.__name__} took {time.time() - start_time:.3f} seconds')\n        return result\n    return wrapper","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def correct_text_spelling(text: str) -> str:\n    spell = SpellChecker()\n    words = re.findall(r'\\b\\w+\\b', text)\n    misspelled = spell.unknown(words)\n    corrected_text = text\n    for word in misspelled:\n        if spell.correction(word):\n            corrected_text = corrected_text.replace(word, spell.correction(word))\n    return corrected_text\n\ndef correct_spelling(df: pd.DataFrame, text_col: str = 'text') -> pd.DataFrame:\n    df_ = df.copy()\n    df_['corrected'] = df_[text_col].apply(correct_text_spelling)\n    return df_['corrected']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features","metadata":{}},{"cell_type":"markdown","source":"## Text Cleaning and Features","metadata":{}},{"cell_type":"code","source":"def clean_text(df: pd.DataFrame) -> pd.DataFrame:\n    df_ = df.copy()\n    \n    df_['clean_text'] = (df_['text'].str.replace('\\n\\n', '') \n                                    .str.replace('\\'s', '')\n                                    .str.replace('[.,?!:;\\'\\\\\\\\\"]', '', regex=True)\n                                    .str.lower()\n                        )\n    \n    return df_['clean_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_corpus(df: pd.DataFrame, text_col: str = 'clean_text') -> list[str]:\n    \"\"\" Returns the text within the chosen column as a list. \"\"\"\n    return df[text_col].to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tfidf(corpus: list[str], vectorizer: Optional[TfidfVectorizer] = None) -> [scipy.sparse.spmatrix | tuple[scipy.sparse.spmatrix, TfidfVectorizer]]:\n    if vectorizer:\n        M = vectorizer.transform(corpus)\n        return M\n    else:\n        vectorizer = TfidfVectorizer(\n                                     lowercase=False,\n                                     sublinear_tf=True,\n                                     stop_words='english',\n                                     ngram_range=(3, 5)\n                                    )\n        \n        M = vectorizer.fit_transform(corpus)\n        \n        return M, vectorizer ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_feature(df: pd.DataFrame, text_col: str = 'clean_text') -> pd.DataFrame:\n    df_ = df.copy()\n    \n    nlp = spacy.load(\"en_core_web_sm\")\n\n    docs = [nlp(t) for t in df_[text_col]]\n    pos_tags = [[token.pos_ for token in doc] for doc in docs]\n    \n    df_['pos_tags'] = pos_tags\n    \n    df_['pos_text'] = df_['pos_tags'].apply(lambda x: ' '.join(x))\n    \n    df_['NOUN_count'] = df_['pos_text'].str.count('NOUN')\n    df_['VERB_count'] = df_['pos_text'].str.count('VERB')\n    df_['ADJ_count'] = df_['pos_text'].str.count('ADJ')\n    df_['ADV_count'] = df_['pos_text'].str.count('ADV')\n    df_['ADP_count'] = df_['pos_text'].str.count('ADP')\n    df_['PRON_count'] = df_['pos_text'].str.count('PRON')\n    df_['PROPN_count'] = df_['pos_text'].str.count('PROPN')\n    df_['PUNCT_count'] = df_['pos_text'].str.count('PUNCT')\n    df_['AUX_count'] = df_['pos_text'].str.count('AUX')\n    df_['NUM_count'] = df_['pos_text'].str.count('NUM')\n    df_['X_count'] = df_['pos_text'].str.count('X')\n    \n    df_ = df_.drop(columns=['pos_tags', 'pos_text'])\n    \n    return df_\n    \ndef stopwords_feature(df: pd.DataFrame, text_col: str = 'clean_text') -> pd.DataFrame:\n    df_ = df.copy()\n    \n    stop_words = set(stopwords.words('english'))\n\n    def stopwords_counter_and_filter(row):\n        words = word_tokenize(row[text_col])\n        stopword_counter = sum(1 for w in words if w.lower() in stop_words)\n        filtered_text = ' '.join(w for w in words if w.lower() not in stop_words)\n        return stopword_counter, filtered_text\n\n    df_[['number_of_stopwords', 'filtered_text']] = df_.apply(stopwords_counter_and_filter, axis=1, result_type='expand')\n\n    return df_\n\ndef caps_to_periods_ratio_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df_ = df.copy()\n    \n    parentheses_count = df_['parentheses_count']\n    caps_count = df_['text'].str.count(r'[A-Z]')\n    df_['caps_to_periods_ratio'] = caps_count / parentheses_count.replace(0, 1) # replace 0 by 1 to avoid deviding by 0\n    \n    return df_['caps_to_periods_ratio']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"def feature_extraction(df: pd.DataFrame, text_col: str = 'clean_text') -> pd.DataFrame:\n    \"\"\" Extracts numeric features based on the chosen text column. \"\"\"\n    df_ = df.copy()\n    \n    # text length\n    df_['num_of_words'] = (df_[text_col].str.replace('\\n\\n', '') # removing row\n                                .str.count(' ')+1)\n    # uniqe words per row\n    df_['text_vocab_size'] = df_[text_col].apply(lambda x : len(set(x.split())))\n    \n    df_['num_of_sentences'] = df_[text_col].str.count('\\.')\n    \n    df_['parentheses_count'] = df_['text'].str.count('\\(|\\)')\n    \n    df_['semicolon_count'] = df_['text'].str.count(';')\n\n    df_['hypen_count'] = df_['text'].str.count('-')\n    \n    df_['dash_count'] = df_['text'].str.count('—')\n    \n    df_['comma_count'] = df_['text'].str.count(',')\n    \n    df_['qm_count'] = df_['text'].str.count('\\?')\n    \n    df_['en_count'] = df_['text'].str.count('!')\n    \n    df_['apostrophe_count'] = df_['text'].str.count('’')\n    \n    df_['paragraph_count'] = df_['text'].str.count('\\n\\n')\n    \n    # extract POS features\n    df_ = pos_feature(df_, text_col)\n    \n    # df_ = stopwords_feature(df_, text_col)\n    \n    # count typos\n    # df_ = typos_count_feature(df_)\n\n    # calculating the ratio between the number of capital letters and periods\n    df_['caps_to_periods_ratio'] = caps_to_periods_ratio_feature(df_)\n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def plt_cm(y_true, y_pred, dataset_type: str, model_name: str):\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots(figsize=(3, 2))\n    class_labels = ['Human', 'LLM']\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"RdPu\", cbar=True,\n                xticklabels=class_labels, yticklabels=class_labels, ax=ax)\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n    ax.set_title(f\"{dataset_type}\\nCM for {model_name}\")\n    plt.show()\n    \ndef plt_roc_auc(y_true, y_score, dataset_type: str, model_name: str):\n    roc_auc_val = roc_auc_score(y_true, y_score)\n    print(f\"\\nROC AUC for {model_name}: {roc_auc_val}\\n\")\n\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n\n    fig, ax = plt.subplots(figsize=(4, 3))\n    ax.plot(fpr, tpr, color='orchid', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.legend(loc=\"best\")\n    ax.set_title(f'{dataset_type}\\nROC Curve for {model_name}')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred, model_name: str):\n    print(\"\\nTRAIN SET\\n\")\n    \n    print(\"Classification report:\")\n    print(classification_report(y_tr, y_tr_pred))\n    \n    plt_cm(y_tr, y_tr_pred, \"Training Set\", model_name)\n    plt_roc_auc(y_tr, y_tr_pred, \"Training Set\", model_name)\n    \n    print(\"\\nVALIDATION SET\\n\")\n    \n    print(\"Classification report:\")\n    print(classification_report(y_val, y_val_pred))\n    \n    plt_cm(y_val, y_val_pred, \"Validation Set\", model_name)\n    plt_roc_auc(y_val, y_val_pred, \"Validation Set\", model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Ensemble Layer","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"def get_df() -> pd.DataFrame:\n    daigt = pd.read_csv(CSV_INPUT_PATH)\n    daigt.rename(columns={'label': 'generated'}, inplace=True)\n    df_ = daigt[daigt['RDizzl3_seven'] == True] # only True ones\n    cols = ['text', 'generated']\n    # df_['text'] = correct_spelling(df_, 'text')\n    \n    df_ = df_[cols]\n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df_with_cleaned_text() -> pd.DataFrame:\n    \"\"\" Returns a dataframe with cleaned text.\n        Note: Sampling by SAMPLE_RATE is applied if SAMPLE (global variable) is True. \"\"\"\n    df = get_df()\n\n    if SAMPLE:\n        df = df.sample(frac=SAMPLE_RATE, random_state=SEED)\n        \n    df['clean_text'] = clean_text(df)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_to_files(tr_pred, val_pred, val2_pred, y_tr, y_val, y_val2, ensemble: VotingClassifier,\n                  vectorizer: TfidfVectorizer, series_name: str, tfidf_type: str):\n    tr_pred = pd.Series(tr_pred, index=y_tr.index, name=series_name)\n    val_pred = pd.Series(val_pred, index=y_val.index, name=series_name)\n    val2_pred = pd.Series(val2_pred, index=y_val2.index, name=series_name)\n    \n    # saving artifacts\n    # saving processed data\n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_tr_pred.csv')\n    tr_pred.to_csv(filepath)\n    \n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_val_pred.csv')\n    val_pred.to_csv(filepath)\n    \n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_val2_pred.csv')\n    val2_pred.to_csv(filepath)\n    \n    # saving objects\n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_ensemble.pickle')\n    pickle.dump(ensemble, open(filepath, 'wb'))\n    \n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_vectorizer.pickle')\n    pickle.dump(vectorizer, open(filepath, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_val_test_split(X: pd.DataFrame, y: pd.Series) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n    X_tr, X_temp, y_tr, y_temp = train_test_split(X, y, test_size=VALIDATION_SHARE, stratify=y, random_state=SEED)\n    X_val, X_val2, y_val, y_val2 = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n\n    return X_tr, X_val, X_val2, y_tr, y_val, y_val2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"ensemble_info = {\n    'lgbm_clf': {'class': LGBMClassifier,\n                 'params': {'random_state':\n                            SEED, 'verbose': -1\n                           }\n                },\n    'cat_clf': {'class': CatBoostClassifier,\n                'params': {'random_seed': SEED,\n                           'verbose': False\n                          }\n               },\n    'lr_clf': {'class': LogisticRegression,\n               'params': {'random_state': SEED,\n                          'max_iter': MAX_ITER\n                         }\n              },\n    'sgd_clf': {'class': SGDClassifier,\n                'params': {'loss': 'log_loss',\n                           'random_state': SEED,\n                           'verbose': 1\n                          }\n               }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ensemble(*clf_names: str) -> VotingClassifier:\n    # training\n    \n    clf_instances = []\n\n    for clf_name in clf_names:\n        clf_class = ensemble_info[clf_name]['class']\n        clf_params = ensemble_info[clf_name]['params']\n\n        clf_instance = clf_class(**clf_params)\n        clf_instances.append((clf_name, clf_instance))\n    \n    ensemble = VotingClassifier(estimators=clf_instances, voting='soft', n_jobs=-1)\n\n    return ensemble","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for the second stage of stack ensemble\ndef get_preds(X_val, df_X_val2, ensemble: VotingClassifier, vectorizer: TfidfVectorizer, is_pos: bool) -> tuple[np.array, np.array]:\n    # train\n    val_pred = ensemble.predict_proba(X_val)[:,1]\n    \n    # validation\n    corpus_val2 = get_corpus(df_X_val2, \"pos_text\" if is_pos else \"clean_text\")\n    X_val2 = tfidf(corpus_val2, vectorizer)\n    val2_pred = ensemble.predict_proba(X_val2)[:,1]\n    \n    return val_pred, val2_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models by Feature Sets","metadata":{}},{"cell_type":"code","source":"@print_run_time\ndef text_tfidf_models() -> tuple[np.array, np.array]:\n    df = get_df_with_cleaned_text()\n\n    # spliting the data\n    X = df.drop(columns=['generated']).copy()\n    y = df['generated'].copy()\n\n    X_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n  \n    # creating corpus and vectorizer\n    corpus_tr = get_corpus(X_tr)\n    tfidf_X_tr, vectorizer = tfidf(corpus_tr)\n\n    ensemble = get_ensemble('lgbm_clf', 'cat_clf', 'lr_clf', 'sgd_clf')\n    ensemble.fit(tfidf_X_tr, y_tr)\n\n    y_tr_pred = ensemble.predict(tfidf_X_tr)\n\n    # intial validation\n    corpus_val = get_corpus(X_val)\n    tfidf_X_val = tfidf(corpus_val, vectorizer)\n    \n    y_val_pred = ensemble.predict(tfidf_X_val)\n    tfidf_tr_pred = ensemble.predict_proba(tfidf_X_tr)[:,1]\n\n    validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred, \"TF-IDF Ensemble\")\n      \n    tfidf_val_pred, tfidf_val2_pred = get_preds(tfidf_X_val, X_val2, ensemble, vectorizer, False)\n    save_to_files(tfidf_tr_pred, tfidf_val_pred, tfidf_val2_pred, y_tr, y_val, y_val2, ensemble, vectorizer, \"text_tfidf_pred\", \"text\")\n    \n    return tfidf_val_pred, tfidf_val2_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@print_run_time\ndef pos_tfidf_models() -> tuple[np.array, np.array]:\n    df = get_df_with_cleaned_text()\n\n    # extracting pos-related feature\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    docs = [nlp(t) for t in df['clean_text']]\n    pos_tags = [[token.pos_ for token in doc] for doc in docs]\n    \n    df['pos_tags'] = pos_tags\n    df['pos_text'] = df['pos_tags'].apply(lambda x: ' '.join(x))\n    \n    # spliting the data\n    X = df.drop(columns=['generated']).copy()\n    y = df['generated'].copy()\n\n    X_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n    \n    corpus_tr = get_corpus(X_tr, \"pos_text\")\n    pos_tfidf_X_tr, vectorizer = tfidf(corpus_tr)\n    \n    ensemble = get_ensemble('lgbm_clf', 'cat_clf', 'lr_clf', 'sgd_clf')\n    ensemble.fit(pos_tfidf_X_tr, y_tr)\n    \n    y_tr_pred = ensemble.predict(pos_tfidf_X_tr)\n\n    # intial validation  \n    pos_corpus_val = get_corpus(X_val, text_col='pos_text')\n    pos_tfidf_X_val = tfidf(pos_corpus_val, vectorizer)\n    \n    y_val_pred = ensemble.predict(pos_tfidf_X_val)\n    pos_tfidf_tr_pred = ensemble.predict_proba(pos_tfidf_X_tr)[:,1]\n    \n    validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred ,\"POS TF-IDF Ensemble\")\n    \n    pos_tfidf_val_pred, pos_tfidf_val2_pred = get_preds(pos_tfidf_X_val, X_val2, ensemble, vectorizer, True)\n    save_to_files(pos_tfidf_tr_pred, pos_tfidf_val_pred, pos_tfidf_val2_pred, y_tr, y_val, y_val2, ensemble, vectorizer, \"pos_tfidf_pred\", \"pos\")\n    \n    return pos_tfidf_val_pred, pos_tfidf_val2_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@print_run_time\ndef numeric_features_models() -> tuple[np.array, np.array]:\n    df = get_df_with_cleaned_text()\n\n    # extracting numeric features\n    df = feature_extraction(df, 'clean_text')\n    \n    # spliting the data\n    cols = ['text','clean_text', 'generated']\n    X = df.drop(columns=cols).copy()\n    y = df['generated'].copy()\n    \n    X_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n    \n    num_ensemble = get_ensemble('lgbm_clf', 'cat_clf', 'lr_clf', 'sgd_clf')\n\n    num_ensemble.fit(X_tr, y_tr)\n    \n    y_tr_pred = num_ensemble.predict(X_tr)\n    y_val_pred = num_ensemble.predict(X_val)\n    \n    # intial validation  \n    validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred ,\"Numeric Ensemble\")\n\n    # for the second stage of stack ensemble\n    # train\n    num_val_pred = num_ensemble.predict_proba(X_val)[:,1]\n    # validation\n    num_val2_pred = num_ensemble.predict_proba(X_val2)[:,1]\n    \n    # transform to pd.Series \n    num_val_pred = pd.Series(num_val_pred, index=y_val.index, name='num_pred')\n    num_val2_pred = pd.Series(num_val2_pred, index=y_val2.index, name='num_pred')\n    \n    # saving artifacts\n    # saving processed data\n    filepath = os.path.join(OUTPUT_PATH, 'num_val_pred.csv')\n    num_val_pred.to_csv(filepath)\n    \n    filepath = os.path.join(OUTPUT_PATH, 'num_val2_pred.csv')\n    num_val2_pred.to_csv(filepath)\n    \n    # saving objects\n    filepath = os.path.join(OUTPUT_PATH, 'num_ensemble.pickle')\n    pickle.dump(num_ensemble, open(filepath, 'wb'))\n    \n    return num_val_pred, num_val2_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Execution and Analysis","metadata":{}},{"cell_type":"code","source":"tfidf_val_pred, tfidf_val2_pred = text_tfidf_models()\npos_val_pred, pos_val2_pred = pos_tfidf_models()\nnum_val_pred, num_val2_pred = numeric_features_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Second Ensemble Layer","metadata":{}},{"cell_type":"code","source":"pred_val = pd.DataFrame({'tfidf_pred': tfidf_val_pred, 'pos_pred': pos_val_pred, 'num_pred': num_val_pred})\npred_val2 = pd.DataFrame({'tfidf_pred': tfidf_val2_pred, 'pos_pred': pos_val2_pred, 'num_pred': num_val2_pred})\n\ndf = get_df()\n\nif SAMPLE:\n    df = df.sample(frac=SAMPLE_RATE, random_state=SEED)\n\nX = df.copy()\ny = df['generated'].copy()\n\ndel df\ngc.collect()\n\n# splitting the data\nX_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n\ndel X_tr, X_val, X_val2, y_tr\ngc.collect()\n\ncat_clf_stack = CatBoostClassifier(random_state=SEED, verbose=False)\n\ncat_clf_stack.fit(pred_val, y_val)\n\ny_val_pred = cat_clf_stack.predict(pred_val)\ny_val2_pred = cat_clf_stack.predict(pred_val2)\n\nfilepath = os.path.join(OUTPUT_PATH, 'cat_clf_stack.pickle')\npickle.dump(cat_clf_stack, open(filepath, 'wb'))\n\ndel filepath, cat_clf_stack\ngc.collect()\n\nvalidation_analysis(y_val, y_val2, y_val_pred, y_val2_pred ,\"CatBoost Over Predictions\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
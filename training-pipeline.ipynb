{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":6920046,"sourceType":"datasetVersion","datasetId":3973543},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7409757,"sourceType":"datasetVersion","datasetId":4309703}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !rm -rf /kaggle/working/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python --version  \n# Python 3.10.12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from typing import Optional, Union\nfrom datetime import datetime\n\nimport os\nimport pickle\nimport gc\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom collections import Counter\n\n!pip install /kaggle/input/pyspellchecker/pyspellchecker-0.8.0-py3-none-any.whl\nfrom spellchecker import SpellChecker\n\nimport spacy\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n# from sklearn.preprocessing import MinMaxScaler\n\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Globals","metadata":{}},{"cell_type":"code","source":"SEED = 1337 # 45090448\nVALIDATION_SHARE = 0.4\nSAMPLE = True # if True then df.sample \nSAMPLE_RATE = 0.002\nCSV_INPUT_PATH = '/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv'\nINPUT_PATH = '/kaggle/input/'\nOUTPUT_PATH = '/kaggle/working/'\nMAX_ITER = 1200","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def printime():\n    now = datetime.now()\n    current_time = now.strftime(\"%H:%M:%S\")\n    print(\"Current Time =\", current_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"def correct_text_spelling(text):\n    spell = SpellChecker()\n    words = re.findall(r'\\b\\w+\\b', text)\n    misspelled = spell.unknown(words)\n    corrected_text = text\n    for word in misspelled:\n        if spell.correction(word):\n            corrected_text = corrected_text.replace(word, spell.correction(word))\n    return corrected_text\n\ndef correct_spelling(df, text_col: str = 'text') -> pd.DataFrame:\n    df_ = df.copy()\n    df_['corrected'] = df_[text_col].apply(correct_text_spelling)\n    return df_['corrected']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df() -> pd.DataFrame:\n    \n    daigt = pd.read_csv(CSV_INPUT_PATH)\n    daigt.rename(columns={'label': 'generated'}, inplace=True)\n    df_ = daigt[daigt['RDizzl3_seven'] == True] # only True ones\n    cols = ['text', 'generated']\n#     df_['text'] = correct_spelling(df_, 'text')\n    \n    df_ = df_[cols]\n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_val_test_split(X: pd.DataFrame, y: pd.Series) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n    X_tr, X_temp, y_tr, y_temp = train_test_split(X, y, test_size=VALIDATION_SHARE, stratify=y, random_state=SEED)\n    X_val, X_val2, y_val, y_val2 = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n\n    return X_tr, X_val, X_val2, y_tr, y_val, y_val2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_corpus(df: pd.DataFrame, text_col: str = 'clean_text') -> list[str]:\n    return df[text_col].to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def right_concat(df: pd.DataFrame, new_df: pd.DataFrame) -> pd.DataFrame:\n    \n    df_ = df.copy()\n    new_df_ = new_df.copy()\n    # forcing the index of the new datasets to match X_train and X_validation\n    new_df_.set_index(df_.index, inplace=True)\n    \n    df_ = pd.concat([df_, new_df_], axis=1)\n    \n    return df_\n\ndef right_concat_split(X_tr: pd.DataFrame,  X_val: pd.DataFrame, \n                 new_df_tr: pd.DataFrame, new_df_val: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    \n    See right_concat function def\n    \n    \"\"\"\n    return right_concat(X_tr, new_df_tr), right_concat(X_val, new_df_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features","metadata":{}},{"cell_type":"markdown","source":"## Text Cleaning and Features","metadata":{}},{"cell_type":"code","source":"# cleaned text\ndef clean_text(df: pd.DataFrame) -> pd.Series:\n    df_ = df.copy()\n    \n    df_['clean_text'] = (df_['text'].str.replace('\\n\\n', '') \n                                    .str.replace('\\'s', '')\n                                    .str.replace('[.,?!:;\\'\\\\\\\\\"]', '', regex=True)\n                                    .str.lower()\n                        )\n    \n    return df_['clean_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tfidf(corpus: list[str], vectorizer: Optional[TfidfVectorizer] = None) -> [pd.DataFrame | tuple[pd.DataFrame, pd.DataFrame]]:\n    \n    if vectorizer:\n        M = vectorizer.transform(corpus)\n        return M\n    \n    else:\n        vectorizer = TfidfVectorizer(\n                                     lowercase=False,\n#                                      token_pattern = None,\n                                     sublinear_tf=True,\n                                     stop_words='english',\n                                     ngram_range=(3,5)\n                                    )\n        \n        M = vectorizer.fit_transform(corpus)\n        \n        return M, vectorizer ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_feature(df: pd.DataFrame, text_col: str = 'clean_text') -> pd.DataFrame:\n    df_ = df.copy()\n    \n    nlp = spacy.load(\"en_core_web_sm\")\n\n    docs = [nlp(t) for t in df_[text_col]]\n    pos_tags = [[token.pos_ for token in doc] for doc in docs]\n    \n    df_['pos_tags'] = pos_tags\n    \n    df_['pos_text'] = df_['pos_tags'].apply(lambda x: ' '.join(x))\n    \n    df_['NOUN_count'] = df_['pos_text'].str.count('NOUN')\n    df_['VERB_count'] = df_['pos_text'].str.count('VERB')\n    df_['ADJ_count'] = df_['pos_text'].str.count('ADJ')\n    df_['ADV_count'] = df_['pos_text'].str.count('ADV')\n    df_['ADP_count'] = df_['pos_text'].str.count('ADP')\n    df_['PRON_count'] = df_['pos_text'].str.count('PRON')\n    df_['PROPN_count'] = df_['pos_text'].str.count('PROPN')\n    df_['PUNCT_count'] = df_['pos_text'].str.count('PUNCT')\n    df_['AUX_count'] = df_['pos_text'].str.count('AUX')\n    df_['NUM_count'] = df_['pos_text'].str.count('NUM')\n    df_['X_count'] = df_['pos_text'].str.count('X')\n    \n    df_ = df_.drop(columns=['pos_tags', 'pos_text'])\n    \n    return df_\n    \ndef stopwords_feature(df, text_col: str = 'clean_text') -> pd.DataFrame:\n    df_ = df.copy()\n    \n    stop_words = set(stopwords.words('english'))\n\n    def stopwords_counter_and_filter(row):\n        words = word_tokenize(row[text_col])\n        stopword_counter = sum(1 for w in words if w.lower() in stop_words)\n        filtered_text = ' '.join(w for w in words if w.lower() not in stop_words)\n        return stopword_counter, filtered_text\n\n    df_[['number_of_stopwords', 'filtered_text']] = df_.apply(stopwords_counter_and_filter, axis=1, result_type='expand')\n\n    return df_\n\ndef caps_to_periods_ratio_feature(df) -> pd.DataFrame:\n    df_ = df.copy()\n    \n    parentheses_count = df_['parentheses_count']\n    caps_count = df_['text'].str.count(r'[A-Z]')\n    df_['caps_to_periods_ratio'] = caps_count / parentheses_count.replace(0, 1) # replace 0 by 1 to avoid deviding by 0\n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"def feature_extraction(df, text_col: str = 'clean_text') -> pd.DataFrame:\n    \"\"\"\n    \n    Extracts text based features for column text_col\n    \n    \"\"\"\n    \n    df_ = df.copy()\n    \n    # text length\n    df_['num_of_words'] = (df_[text_col].str.replace('\\n\\n', '') # removing row\n                                .str.count(' ')+1)\n    # uniqe words per row\n    df_['text_vocab_size'] = df_[text_col].apply(lambda x : len(set(x.split())))\n    \n    df_['num_of_sentences'] = df_[text_col].str.count('\\.')\n    \n    df_['parentheses_count'] = df_['text'].str.count('\\(|\\)')\n    \n    df_['semicolon_count'] = df_['text'].str.count(';')\n\n    df_['hypen_count'] = df_['text'].str.count('-')\n    \n    df_['dash_count'] = df_['text'].str.count('—')\n    \n    df_['comma_count'] = df_['text'].str.count(',')\n    \n    df_['qm_count'] = df_['text'].str.count('\\?')\n    \n    df_['en_count'] = df_['text'].str.count('!')\n    \n    df_['apostrophe_count'] = df_['text'].str.count('’')\n    \n    df_['paragraph_count'] = df_['text'].str.count('\\n\\n')\n    \n    # extract POS features\n    df_ = pos_feature(df_, text_col)\n    \n    # df_ = stopwords_feature(df_, text_col)\n    \n    # count typos\n#     df_ = typos_count_feature(df_)\n\n    # calculate the ratio between the number of capital letters and periods\n    df_ = caps_to_periods_ratio_feature(df_)\n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def plt_cm(y_true, y_pred, set_type, model_name):\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots(figsize=(3, 2))\n    class_labels = ['Human', 'LLM']\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"RdPu\", cbar=True,\n                xticklabels=class_labels, yticklabels=class_labels, ax=ax)\n#     title_font = {'family': 'serif',\n#                   'color': 'blue',\n#                   'weight': 'bold',\n#                   'size': 14\n#                  }\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n    ax.set_title(f\"{set_type}\\nCM for {model_name}\") # fontdict=title_font\n    plt.show()\n    \ndef plt_roc_auc(y_true, y_score, set_type, model_name):\n    roc_auc_val = roc_auc_score(y_true, y_score)\n    print(f\"\\nROC AUC for {model_name}: {roc_auc_val}\\n\")\n\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n\n    fig, ax = plt.subplots(figsize=(4, 3))\n    ax.plot(fpr, tpr, color='orchid', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.legend(loc=\"best\")\n    ax.set_title(f'{set_type}\\nROC Curve for {model_name}')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation functions\ndef validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred, model_name):\n    \n    print(\"\\nTRAIN SET\\n\")\n    \n    print(\"Classification report:\")\n    print(classification_report(y_tr, y_tr_pred))\n    \n    plt_cm(y_tr, y_tr_pred, \"Training Set\", model_name)\n    plt_roc_auc(y_tr, y_tr_pred, \"Training Set\", model_name)\n    \n    print(\"\\nVALIDATION SET\\n\")\n    \n    print(\"Classification report:\")\n    print(classification_report(y_val, y_val_pred))\n    \n    plt_cm(y_val, y_val_pred, \"Validation Set\", model_name)\n    plt_roc_auc(y_val, y_val_pred, \"Validation Set\", model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Ensemble Layer","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"def get_df_with_cleaned_text() -> pd.DataFrame:\n    df = get_df()\n\n    if SAMPLE:\n        df = df.sample(frac=SAMPLE_RATE, random_state=SEED)\n        \n    df['clean_text'] = clean_text(df)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_to_files(tr_pred, val_pred, val2_pred, y_tr, y_val, y_val2, ensemble, vectorizer, series_name, tfidf_type):\n    tr_pred = pd.Series(tr_pred, index=y_tr.index, name=series_name)\n    val_pred = pd.Series(val_pred, index=y_val.index, name=series_name)\n    val2_pred = pd.Series(val2_pred, index=y_val2.index, name=series_name)\n    \n    # saving artifacts\n    # saving processed data\n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_tr_pred.csv')\n    tr_pred.to_csv(filepath)\n    \n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_val_pred.csv')\n    val_pred.to_csv(filepath)\n    \n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_val2_pred.csv')\n    val2_pred.to_csv(filepath)\n    \n    # saving objects\n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_ensemble.pickle')\n    pickle.dump(ensemble, open(filepath, 'wb'))\n    \n    filepath = os.path.join(OUTPUT_PATH, f'{tfidf_type}_tfidf_vectorizer.pickle')\n    pickle.dump(vectorizer, open(filepath, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def get_vectorizer(X_tr, is_pos) -> tuple[pd.DataFrame, pd.DataFrame]:\n    corpus_tr = get_corpus(X_tr, \"pos_text\" if is_pos else \"clean_text\")\n    X_tr, vectorizer = tfidf(corpus_tr)\n        \n    return X_tr, vectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ensemble() -> VotingClassifier:\n    # training\n    \n    # svm_clf = SVC(random_state=SEED, verbose=False)\n    lgbm_clf = LGBMClassifier(random_state=SEED, verbose=-1)\n    cat_clf = CatBoostClassifier(random_seed=SEED, verbose=False)\n    lr_clf = LogisticRegression(random_state=SEED, max_iter=MAX_ITER)\n    sgd_clf = SGDClassifier(loss='log_loss', random_state=SEED, verbose=1)\n    \n    model_list = [\n                    ('lgbm_clf',lgbm_clf), \n                    ('cat_clf', cat_clf),\n                    ('lr_clf', lr_clf),\n                    ('sgd_clf', sgd_clf)\n                 ]\n    \n    ensemble = VotingClassifier(estimators=model_list, voting='soft', n_jobs=-1)\n    \n    return ensemble","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ensemble_predict_and_proba(ensemble, tfidf_X_val, tfidf_X_tr):\n    y_val_pred = ensemble.predict(tfidf_X_val)\n    tfidf_tr_pred = ensemble.predict_proba(tfidf_X_tr)[:,1]\n        \n    return y_val_pred, tfidf_tr_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds(X_val, df_X_val2, ensemble, vectorizer, is_pos) -> tuple[pd.Series, pd.Series]:\n    # for the second stage of stack ensemble\n    # train\n    val_pred = ensemble.predict_proba(X_val)[:,1]\n    \n    # validation\n    corpus_val2 = get_corpus(df_X_val2, \"pos_text\" if is_pos else \"clean_text\")\n    X_val2 = tfidf(corpus_val2, vectorizer)\n    val2_pred = ensemble.predict_proba(X_val2)[:,1]\n    \n    return val_pred, val2_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models by Feature Sets","metadata":{}},{"cell_type":"code","source":"def text_tfidf_models():\n    df = get_df_with_cleaned_text()\n\n    # spliting the data\n    X = df.drop(columns=['generated']).copy()\n    y = df['generated'].copy()\n\n    X_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n  \n    tfidf_X_tr, vectorizer = get_vectorizer(X_tr, False)\n\n    ensemble = get_ensemble()\n    ensemble.fit(tfidf_X_tr, y_tr)\n\n    y_tr_pred = ensemble.predict(tfidf_X_tr)\n\n    # intial validation\n    corpus_val = get_corpus(X_val)\n    tfidf_X_val = tfidf(corpus_val, vectorizer)\n    \n    y_val_pred, tfidf_tr_pred = get_ensemble_predict_and_proba(ensemble, tfidf_X_val, tfidf_X_tr)\n\n    validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred, \"TF-IDF Ensemble\")\n      \n    tfidf_val_pred, tfidf_val2_pred = get_preds(tfidf_X_val, X_val2, ensemble, vectorizer, False)\n    save_to_files(tfidf_tr_pred, tfidf_val_pred, tfidf_val2_pred, y_tr, y_val, y_val2, ensemble, vectorizer, \"text_tfidf_pred\", \"text\")\n    \n    return tfidf_tr_pred, tfidf_val_pred, tfidf_val2_pred, ensemble, vectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_tfidf_models():\n    df = get_df_with_cleaned_text()\n\n    # extracting pos-related feature\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    docs = [nlp(t) for t in df['clean_text']]\n    pos_tags = [[token.pos_ for token in doc] for doc in docs]\n    \n    df['pos_tags'] = pos_tags\n    df['pos_text'] = df['pos_tags'].apply(lambda x: ' '.join(x))\n    \n    # spliting the data\n    X = df.drop(columns=['generated']).copy()\n    y = df['generated'].copy()\n\n    X_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n    \n    pos_tfidf_X_tr, vectorizer = get_vectorizer(X_tr, True)\n    \n    ensemble = get_ensemble()\n    ensemble.fit(pos_tfidf_X_tr, y_tr)\n    \n    y_tr_pred = ensemble.predict(pos_tfidf_X_tr)\n\n    # intial validation  \n    pos_corpus_val = get_corpus(X_val, text_col='pos_text')\n    pos_tfidf_X_val = tfidf(pos_corpus_val, vectorizer)\n    \n    y_val_pred, pos_tfidf_tr_pred = get_ensemble_predict_and_proba(ensemble, pos_tfidf_X_val, pos_tfidf_X_tr)\n\n    validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred ,\"POS TF-IDF Ensemble\")\n    \n    pos_tfidf_val_pred, pos_tfidf_val2_pred = get_preds(pos_tfidf_X_val, X_val2, ensemble, vectorizer, True)\n    save_to_files(pos_tfidf_tr_pred, pos_tfidf_val_pred, pos_tfidf_val2_pred, y_tr, y_val, y_val2, ensemble, vectorizer, \"pos_tfidf_pred\", \"pos\")\n    \n    return pos_tfidf_tr_pred, pos_tfidf_val_pred, pos_tfidf_val2_pred, ensemble, vectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def numeric_features_models():\n    df = get_df_with_cleaned_text()\n\n    # extracting numeric features\n    df = feature_extraction(df, 'clean_text')\n    \n    # spliting the data\n    cols = ['text','clean_text', 'generated']\n    X = df.drop(columns=cols).copy()\n    y = df['generated'].copy()\n    \n    X_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n    \n    num_ensemble = get_ensemble()\n\n    num_ensemble.fit(X_tr, y_tr)\n    \n    y_tr_pred = num_ensemble.predict(X_tr)\n    y_val_pred = num_ensemble.predict(X_val)\n    \n    # intial validation  \n    validation_analysis(y_tr, y_val, y_tr_pred, y_val_pred ,\"Numeric Ensemble\")\n\n    # for the second stage of stack ensemble\n    # train\n    num_val_pred = num_ensemble.predict_proba(X_val)[:,1]\n    # validation\n    num_val2_pred = num_ensemble.predict_proba(X_val2)[:,1]\n    \n    # transform to pd.Series \n    num_val_pred = pd.Series(num_val_pred, index=y_val.index, name='num_pred')\n    num_val2_pred = pd.Series(num_val2_pred, index=y_val2.index, name='num_pred')\n    \n    # saving artifacts\n    # saving processed data\n    filepath = os.path.join(OUTPUT_PATH, 'num_val_pred.csv')\n    num_val_pred.to_csv(filepath)\n    \n    filepath = os.path.join(OUTPUT_PATH, 'num_val2_pred.csv')\n    num_val2_pred.to_csv(filepath)\n    \n    # saving objects\n    filepath = os.path.join(OUTPUT_PATH, 'num_ensemble.pickle')\n    pickle.dump(num_ensemble, open(filepath, 'wb'))\n    \n    return num_val_pred, num_val2_pred, num_ensemble","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Execution and Analysis","metadata":{}},{"cell_type":"code","source":"printime()\ntfidf_tr_pred, tfidf_val_pred, tfidf_val2_pred, tfidf_ensemble, tfidf_vectorizer = text_tfidf_models()\nprintime()\npos_tr_pred, pos_val_pred, pos_val2_pred, pos_ensemble, pos_vectorizer = pos_tfidf_models()\nprintime()\nnum_val_pred, num_val2_pred, num_ensemble = numeric_features_models()\nprintime()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Second Ensemble Layer","metadata":{}},{"cell_type":"code","source":"pred_val = pd.DataFrame({'tfidf_pred': tfidf_val_pred, 'pos_pred': pos_val_pred, 'num_pred': num_val_pred})\npred_val2 = pd.DataFrame({'tfidf_pred': tfidf_val2_pred, 'pos_pred': pos_val2_pred, 'num_pred': num_val2_pred})\n\n# importing for intial y_tr, y_val\ndf = get_df()\n\nif SAMPLE:\n    df = df.sample(frac=SAMPLE_RATE, random_state=SEED)\n\nX = df.copy()\ny = df['generated'].copy()\n\ndel df\ngc.collect()\n\n# splitting the data\nX_tr, X_val, X_val2, y_tr, y_val, y_val2 = train_val_test_split(X, y)\n\ndel X_tr, X_val, X_val2, y_tr\ngc.collect()\n\ncat_clf_stack = CatBoostClassifier(random_state=SEED, verbose=False)\n\ncat_clf_stack.fit(pred_val, y_val)\n\ny_val_pred = cat_clf_stack.predict(pred_val)\ny_val2_pred = cat_clf_stack.predict(pred_val2)\n\nfilepath = os.path.join(OUTPUT_PATH, 'cat_clf_stack.pickle')\npickle.dump(cat_clf_stack, open(filepath, 'wb'))\n\ndel filepath, cat_clf_stack\ngc.collect()\n\nvalidation_analysis(y_val, y_val2, y_val_pred, y_val2_pred ,\"CatBoost Over Predictions\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printime()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}